q{1} ------> r{1,5,3,1,3,7} ------> c{1,5,3,7}
             q{}=r{}-c{}

q{1,5,3,7} ------> r{1,3,4,2,7,9} ------> c{1,5,3,7,4,2,9}
                   q{}

q{4,2,9} ------> r{1,5,3,1,3,7} ------> c{1,5,3,7}
             q{}


from utils.utils import CustomWebDriver, async_get
from bs4 import BeautifulSoup
from lxml import html
from selenium.common.exceptions import WebDriverException, TimeoutException
from urllib.parse import urljoin
import time, threading, asyncio


class WebScout:
    """ """

    base_url = None
    queue = set()
    crawled = set()

    def __init__(self, base_url: str, crawled: set):
        if not WebScout.base_url:
            WebScout.base_url = base_url
            WebScout.queue.add(base_url)

        WebScout.crawled.update(crawled)

    @staticmethod
    def crawl(url: str, *, lock: threading.Lock, output: set = None):
        # start inspecting
        res = [
            urljoin(url, i.strip())
            for i in WebScout.inspect(
                url,
                helper="lxml",
                xpath="//a[contains(@href,'shop')and not(contains(@href,'add-to-cart'))and not(contains(@href,'#'))]/@href",
            )
        ]  # founded urls

        # update queue and crawled
        with lock:
            WebScout.queue.remove(url)  # remove inspected url
            WebScout.queue.update(
                i for i in res if i not in WebScout.crawled
            )  # put new urls in queue for inspecting
            WebScout.crawled.add(url)  # put inspected url to crawled list
            WebScout.crawled.update(res)  # also update founded urls to crawled list
            output.update(WebScout.crawled)  # update (for writing to file)

    @staticmethod
    def inspect(
        url: str,
        *,
        helper: str,
        xpath: str = None,
        tag: str = None,
        attr: str = None,
        filter_attrs: dict = None,
        **kwargs,
    ):
        """
        Return list of inspected HTML tag values from given URL. Avoid passing broken URL when using **Selenium**.

        Parameters
        ---
        url : str
            Source domain
        helper : str
            Support **Selenium**, **Beautifulsoup** and **lxml**
        xpath : str, optional
            Use with **Selenium** or **lxml** (default: **None**)
        tag : str, optional
            Inspected tag, use with **Beautifulsoup** (default: **None**)
        attr: str, optional
            Indicate selected attribute of a tag, use with **Beautifulsoup** (default: **None**)
        filter_attrs: dict, optional
            Specific attributes with exact value, use with **Beautifulsoup** (default: **None**)
        kwargs: any, optional
            Use with **Beautifulsoup** (default: **None**)
        """

        data = []
        resp = None
        final_e = None  # final exception (if caught)
        turns = 3  # retry turns
        driver = None  # for selenium
        helper = helper.lower()

        # check prerequisite and prepare reps
        if helper in ["beautifulsoup", "beautifulsoup4", "bs4"]:
            if xpath:
                print(
                    "Only use tag, attr, filter_attrs and kwargs related to Beautifulsoup."
                )
                return
        elif helper == "lxml":
            if any([tag, attr, filter_attrs, kwargs]):
                print("Only use xpath along with lxml.")
                return
        elif helper == "selenium":
            if any([tag, attr, filter_attrs, kwargs]) and xpath:
                print(
                    "Avoid tag, attr, filter_attrs and kwargs related to Beautifulsoup when using xpath, and vice versa."
                )
                return
        else:
            print("Only support Selenium, Beautifulsoup and lxml libraries.")
            return

        # handle retry for static content
        def retry_load():
            nonlocal final_e, resp, turns, driver

            if helper == "selenium":
                try:
                    driver = CustomWebDriver()
                    driver.set_page_load_timeout(15)
                    driver.get(url)
                except WebDriverException as e:  # connection lost
                    final_e = e
                    driver.quit()
                    driver = None
                    return
                except TimeoutException as e:
                    print(f"{e}. Retrying...")
                    while turns > 0:
                        try:
                            driver = CustomWebDriver()
                            driver.set_page_load_timeout(15)
                            driver.get(url)
                            break
                        except Exception as e:
                            print(f"{e}. Retrying...")
                            final_e = e
                            driver.quit()
                            driver = None  # reset if fail again
                        turns -= 1
                        time.sleep(5)

            else:
                pass

        # main work
        if helper == "selenium":  # dynamic content
            if not any([tag, attr, filter_attrs, kwargs, xpath]):
                print("Missing lxml or Beautifulsoup related parameters.")
                return

            retry_load()

            if final_e or not driver:
                if turns == 3:  # connection lost case
                    print(final_e)
                    print("(Make sure the connection and webdriver remains stable)")
                    return
                print("Failed 3 times.", {final_e})
                return
            if not driver:
                print("Failed 3 times.", {final_e})
                return

            if xpath:  # use lxml
                page = html.fromstring(driver.page_source)
                data = page.xpath(xpath)
                driver.quit()
                return data
            else:  # use bs4
                soup = BeautifulSoup(driver.page_source, "html.parser")
                tags = soup.find_all(tag, filter_attrs, **kwargs)

                for i in tags:
                    data.append(i.get(attr) if attr else i)

                driver.quit()
                return data
        elif helper == "lxml":  # static content
            resp = asyncio.run(async_get(url))

            if not resp:
                print(f"Can not inspect {url}. {resp}")
                return

            page = html.fromstring(resp.content)

            if xpath:
                return page.xpath(xpath)

            return html.tostring(page, pretty_print=True, encoding="unicode")
        else:  # static content
            resp = async_get(url)

            if not resp:
                print(f"Can not inspect {url}. {resp}")
                return

            soup = BeautifulSoup(resp.content, "html.parser")
            tags = soup.find_all(tag, filter_attrs, **kwargs)

            for i in tags:
                data.append(i.get(attr) if attr else i)

            return data


from webexplorer import WebScout
from utils import runtime, Cursor
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading, httpx, asyncio
from lxml import html


@runtime
def main():
    main_page = "https://scrapeme.live/shop/"
    crawled = set()
    lock = threading.Lock()

    # initial step
    WebScout(main_page, crawled)

    # start the work
    with httpx.Client(
        timeout=20.0,
        follow_redirects=True,
        headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:139.0) Gecko/20100101 Firefox/139.0",
            "Connection": "keep-alive",
        },
    ) as client:
        while WebScout.queue:
            # limit thread workers
            workers = list(WebScout.queue)[:100]

            with ThreadPoolExecutor(max_workers=100) as ex:
                futures = [
                    ex.submit(
                        WebScout.crawl,
                        url=i,
                        lock=lock,
                        output=crawled,
                        client=client,
                    )
                    for i in workers
                ]

                for _ in as_completed(futures):
                    print(
                        f"Checklist remains {len(WebScout.queue)} | Passed {len(WebScout.crawled)}"
                    )

    print(f"Found {len(crawled)} urls.", Cursor.clear)
    print(Cursor.reveal)

============================================

import asyncio
import httpx
from concurrent.futures import ThreadPoolExecutor
from lxml import html

# SETTINGS
CONCURRENCY_LIMIT = 20
PARSER_THREADS = 10
URLS = ["https://example.com/page1", "https://example.com/page2", ...]  # your URLs

# QUEUE
response_queue = asyncio.Queue()


# ----------------------
# ASYNC FETCHER (Producer)
# ----------------------
async def fetch_url(url, client, semaphore):
    async with semaphore:
        try:
            resp = await client.get(url, timeout=10)
            resp.raise_for_status()
            await response_queue.put((url, resp.text))
        except Exception as e:
            print(f"[x] Failed to fetch {url} >> {e}")


async def producer(urls):
    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)
    async with httpx.AsyncClient() as client:
        tasks = [fetch_url(url, client, semaphore) for url in urls]
        await asyncio.gather(*tasks)

    # Signal consumers that fetching is done
    for _ in range(PARSER_THREADS):
        await response_queue.put(None)


# ----------------------
# THREADED PARSER (Consumer)
# ----------------------
def parse_html(url, raw_html):
    try:
        tree = html.fromstring(raw_html)
        title = tree.xpath("//title/text()")[0] if tree.xpath("//title/text()") else "No title"
        print(f"[âœ“] Parsed {url} -> {title}")
        return (url, title)
    except Exception as e:
        print(f"[x] Parse error for {url} >> {e}")
        return (url, None)


async def consumer(executor):
    while True:
        item = await response_queue.get()
        if item is None:
            break  # signal to end

        url, raw_html = item
        await asyncio.to_thread(parse_html, url, raw_html)


# ----------------------
# MAIN ENTRY
# ----------------------
async def main():
    executor = ThreadPoolExecutor(max_workers=PARSER_THREADS)
    consumers = [consumer(executor) for _ in range(PARSER_THREADS)]

    await asyncio.gather(
        producer(URLS),
        *consumers,
    )

asyncio.run(main())
